{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from frozen_lake import *\n",
    "env = FrozenLakeEnvMultigoal(goal=2)\n",
    "#print(env.__doc__)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np, numpy.random as nr, gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Seed RNGs so you get the same printouts as me\n",
    "env.seed(0); from gym.spaces import prng; prng.seed(10)\n",
    "\n",
    "NUM_ITER = 75\n",
    "gamma = 0.95\n",
    "\n",
    "\n",
    "\n",
    "class MDP(object):\n",
    "    def __init__(self, env):\n",
    "        P, nS, nA, desc = MDP.env2mdp(env)\n",
    "        self.P = P # state transition and reward probabilities, explained below\n",
    "        self.nS = nS # number of states\n",
    "        self.nA = nA # number of actions\n",
    "        self.desc = desc # 2D array specifying what each grid cell means (used for plotting)\n",
    "        self.env = env\n",
    "        self.T = self.get_transition_matrix()\n",
    "\n",
    "    def env2mdp(env):\n",
    "        return {s : {a : [tup[:3] for tup in tups] for (a, tups) in a2d.items()} for (s, a2d) in env.P.items()}, env.nS, env.nA, env.desc\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        \"\"\"Return a matrix with index S,A,S' -> P(S'|S,A)\"\"\"\n",
    "        T = np.zeros([self.nS, self.nA, self.nS])\n",
    "        for s in range(self.nS):\n",
    "            for a in range(self.nA):\n",
    "                transitions = self.P[s][a]\n",
    "                s_a_s = {t[1]:t[0] for t in transitions}\n",
    "                for s_prime in range(self.nS):\n",
    "                    if s_prime in s_a_s:\n",
    "                        T[s, a, s_prime] = s_a_s[s_prime]\n",
    "        return T\n",
    "    \n",
    "mdp = MDP(env)\n",
    "\n",
    "\n",
    "#print(\"mdp.P is a two-level dict where the first key is the state and the second key is the action.\")\n",
    "#print(\"The 2D grid cells are associated with indices [0, 1, 2, ..., 15] from left to right and top to down, as in\")\n",
    "#print(np.arange(16).reshape(4,4))\n",
    "#print(\"mdp.P[state][action] is a list of tuples (probability, nextstate, reward).\\n\")\n",
    "#print(\"For example, state 0 is the initial state, and the transition information for s=0, a=0 is \\nP[0][0] =\", mdp.P[0][0], \"\\n\")\n",
    "#print(\"As another example, state 5 corresponds to a hole in the ice, which transitions to itself with probability 1 and reward 0.\")\n",
    "#print(\"P[5][0] =\", mdp.P[5][0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_value(mdp, gamma, reward,  threshold=1e-4):\n",
    "    \"\"\"\n",
    "    Find the optimal value function via value iteration with the max-ent Bellman backup \n",
    "    given at https://graphics.stanford.edu/projects/gpirl/gpirl_supplement.pdf.\n",
    "\n",
    "    reward: Vector of rewards for each state.\n",
    "    threshold: Convergence threshold.\n",
    "    gamma: MDP gamma factor. float.\n",
    "    -> Array of values for each state\n",
    "    \"\"\"\n",
    "\n",
    "    v = np.zeros(mdp.nS)\n",
    "\n",
    "    diff = float(\"inf\")\n",
    "    while diff > threshold:\n",
    "        v_prev = np.copy(v)\n",
    "        diff = 0\n",
    "        for s in range(mdp.nS):\n",
    "            v_s_new = 0\n",
    "            for a in range(mdp.nA):\n",
    "                v_s_new += np.exp(np.dot(mdp.T[s, a, :], reward[s] + gamma*v_prev))\n",
    "            v[s] = np.log(v_s_new)\n",
    "            new_diff = abs(v_prev[s] - v[s])\n",
    "            if new_diff > diff:\n",
    "                diff = new_diff         \n",
    "    return v\n",
    "\n",
    "\n",
    "def compute_policy(mdp, gamma, reward=None, V=None, threshold=1e-4):\n",
    "    \n",
    "    if reward is None and V is None: raise Exception('Cannot compute V: no reward provided')\n",
    "    if V is None: V = compute_value(mdp, gamma, reward, threshold=threshold)\n",
    "\n",
    "    policy = np.zeros((mdp.nS, mdp.nA))\n",
    "    for s in range(mdp.nS):\n",
    "        for a in range(mdp.nA):\n",
    "            policy[s,a] = np.exp(r1[s] - V[s] + np.dot(mdp.T[s, a,:], gamma * V))\n",
    "    \n",
    "    return policy\n",
    "\n",
    "\n",
    "def generate_trajectories(mdp, policy, T=20, D=50):\n",
    "    s = mdp.env.reset()\n",
    "    \n",
    "    trajectories = np.zeros([D, T, 2]).astype(int)\n",
    "    \n",
    "    for d in range(D):\n",
    "        for t in range(T):\n",
    "            action = np.random.choice(range(mdp.nA), p=policy[s, :])\n",
    "            trajectories[d, t, :] = [s, action]\n",
    "            s, _, _, _ = mdp.env.step(action)\n",
    "        s = mdp.env.reset()\n",
    "    \n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mdp1 = MDP(FrozenLakeEnvMultigoal(is_slippery=False, goal=1))\n",
    "\n",
    "\n",
    "t1 = mdp1.get_transition_matrix()\n",
    "\n",
    "r1 = np.zeros(64)\n",
    "r1[63] = 1.0\n",
    "\n",
    "policy1 = compute_policy(mdp1, gamma, r1, threshold=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trajectories1 = generate_trajectories(mdp1, policy1, T=50, D=100)\n",
    "\n",
    "trajectories1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_irl_log_likelihood(mdp, gamma, trajectories, V, r):\n",
    "\n",
    "    L_D = 0\n",
    "\n",
    "    for traj in trajectories:\n",
    "        for (s, a) in traj:\n",
    "            L_D += r[s] - V[s] + np.dot(mdp.T[s,a,:], gamma * V)\n",
    "    \n",
    "    return L_D\n",
    "\n",
    "\n",
    "\n",
    "def compute_s_a_visitations(mdp, gamma, trajectories):\n",
    "    mu_hat_sa = np.zeros((mdp1.nS, mdp1.nA))\n",
    "    v_hat_s = np.zeros((mdp1.nS))\n",
    "    for traj in trajectories:\n",
    "        for (s, a) in traj:\n",
    "            mu_hat_sa[s, a] += 1\n",
    "            v_hat_s[s] += 1\n",
    "\n",
    "            v_hat_s -= gamma * mdp1.T[s,a,:]\n",
    "            # Same as the line above but slower:\n",
    "            #for (s_prime, p_transition) in enumerate(t1[s,a,:]):\n",
    "            #    v_hat_s[s_prime] -= gamma * p_transition\n",
    "    return(mu_hat_sa, v_hat_s)\n",
    "        \n",
    "        \n",
    "def compute_mu_tilda(mdp, gamma, V, policy, v_hat_s, mu_tilda = None, threshold = 1e-4):\n",
    "    \"\"\"\n",
    "    Computes occupancy measure of a MDP under a given policy -- \n",
    "    the expected discounted number of times that policy Ï€ visits state s.\n",
    "    \"\"\"\n",
    "    assert V.shape[0] == mdp.nS\n",
    "    assert policy.shape == (mdp.nS, mdp.nA)    \n",
    "    \n",
    "    if mu_tilda is None: mu_tilda =  np.zeros((mdp.nS, mdp.nA))\n",
    "    else: mu_tilda = np.tile(mu_tilda, (mdp.nA, 1))\n",
    "    \n",
    "    diff = float(\"inf\")\n",
    "    \n",
    "    while diff > threshold:\n",
    "        mu_tilda_new = np.copy(mu_tilda)\n",
    "        \n",
    "        for s_prime in range(mdp.nS):\n",
    "            for a_prime in range(mdp.nA):\n",
    "                mu_tilda_new[s_prime, a_prime] = (policy[s_prime, a_prime] * \n",
    "                                                 (v_hat_s[s_prime] + \n",
    "                                                  np.sum(gamma * mdp.T[:,:,s_prime] * mu_tilda)))\n",
    "        \n",
    "        diff = np.amax(abs(mu_tilda - mu_tilda_new))    \n",
    "        mu_tilda = np.copy(mu_tilda_new)\n",
    "    \n",
    "    return np.sum(mu_tilda, 1)\n",
    "\n",
    "\n",
    "def irl_log_likelihood_and_grad(mdp, gamma, r, trajectories):\n",
    "    \n",
    "    V = compute_value(mdp1, gamma, r)\n",
    "    # IRL log likelihood term\n",
    "    L_D = compute_irl_log_likelihood(mdp, gamma, trajectories, V, r)\n",
    "    \n",
    "    # IRL log likelihood gradient w.r.t reward\n",
    "    policy = compute_policy(mdp1, gamma, V=V) \n",
    "    mu_hat, v_hat_s = compute_s_a_visitations(mdp, gamma, trajectories)\n",
    "    mu_tilda = compute_mu_tilda(mdp, gamma, V, policy, v_hat_s)\n",
    "    \n",
    "    dL_D_dr = np.sum(mu_hat,1) - mu_tilda\n",
    "    \n",
    "    return L_D, dL_D_dr\n",
    "\n",
    "irl_log_likelihood_and_grad(mdp1, gamma, r1, trajectories1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
