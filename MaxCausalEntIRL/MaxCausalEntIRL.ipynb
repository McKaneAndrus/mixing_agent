{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from frozen_lake import *\n",
    "env = FrozenLakeEnvMultigoal(goal=2)\n",
    "#print(env.__doc__)\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np, numpy.random as nr, gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Seed RNGs so you get the same printouts as me\n",
    "env.seed(0); from gym.spaces import prng; prng.seed(10)\n",
    "\n",
    "NUM_ITER = 75\n",
    "gamma = 0.95\n",
    "\n",
    "\n",
    "\n",
    "class MDP(object):\n",
    "    def __init__(self, env):\n",
    "        P, nS, nA, desc = MDP.env2mdp(env)\n",
    "        self.P = P # state transition and reward probabilities, explained below\n",
    "        self.nS = nS # number of states\n",
    "        self.nA = nA # number of actions\n",
    "        self.desc = desc # 2D array specifying what each grid cell means (used for plotting)\n",
    "        self.env = env\n",
    "        self.T = self.get_transition_matrix()\n",
    "\n",
    "    def env2mdp(env):\n",
    "        return {s : {a : [tup[:3] for tup in tups] for (a, tups) in a2d.items()} for (s, a2d) in env.P.items()}, env.nS, env.nA, env.desc\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        \"\"\"Return a matrix with index S,A,S' -> P(S'|S,A)\"\"\"\n",
    "        T = np.zeros([self.nS, self.nA, self.nS])\n",
    "        for s in range(self.nS):\n",
    "            for a in range(self.nA):\n",
    "                transitions = self.P[s][a]\n",
    "                s_a_s = {t[1]:t[0] for t in transitions}\n",
    "                for s_prime in range(self.nS):\n",
    "                    if s_prime in s_a_s:\n",
    "                        T[s, a, s_prime] = s_a_s[s_prime]\n",
    "        return T\n",
    "    \n",
    "mdp = MDP(env)\n",
    "\n",
    "\n",
    "#print(\"mdp.P is a two-level dict where the first key is the state and the second key is the action.\")\n",
    "#print(\"The 2D grid cells are associated with indices [0, 1, 2, ..., 15] from left to right and top to down, as in\")\n",
    "#print(np.arange(16).reshape(4,4))\n",
    "#print(\"mdp.P[state][action] is a list of tuples (probability, nextstate, reward).\\n\")\n",
    "#print(\"For example, state 0 is the initial state, and the transition information for s=0, a=0 is \\nP[0][0] =\", mdp.P[0][0], \"\\n\")\n",
    "#print(\"As another example, state 5 corresponds to a hole in the ice, which transitions to itself with probability 1 and reward 0.\")\n",
    "#print(\"P[5][0] =\", mdp.P[5][0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x1,x2):\n",
    "    max_x = np.amax((x1,x2))\n",
    "    min_x = np.amin((x1,x2))\n",
    "    return max_x + np.log(1+np.exp(min_x - max_x))\n",
    "\n",
    "def compute_value_boltzmann(mdp, gamma, reward, horizon = None, threshold=1e-4):\n",
    "    \"\"\"\n",
    "    Find the optimal value function via value iteration with the max-ent Bellman backup \n",
    "    given at https://graphics.stanford.edu/projects/gpirl/gpirl_supplement.pdf.\n",
    "\n",
    "    reward: Vector of rewards for each state.\n",
    "    threshold: Convergence threshold.\n",
    "    gamma: MDP gamma factor. float.\n",
    "    -> Array of values for each state\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    if horizon is not None:\n",
    "        gamma = 1\n",
    "        print('here')\n",
    "    \n",
    "    v = r\n",
    "    t = 0\n",
    "    diff = float(\"inf\")\n",
    "    while diff > threshold:\n",
    "        v_prev = np.copy(v)\n",
    "        diff = 0\n",
    "        for s in range(mdp.nS):\n",
    "            v_s_new = 0\n",
    "            for a in range(mdp.nA):\n",
    "                if a == 0:\n",
    "                    v_s_new += reward[s] + gamma * np.dot(mdp.T[s, a, :], v_prev)  \n",
    "                else:\n",
    "                    v_s_new = softmax(v_s_new, reward[s] + gamma * np.dot(mdp.T[s, a, :], v_prev))\n",
    "            \n",
    "                if np.sum(np.isnan(v_s_new)) > 0: \n",
    "                    raise Exception('NaN encountered at iteration ', t, 'state',s, ' action ', a)\n",
    "            \n",
    "            v[s] = v_s_new\n",
    "\n",
    "            \n",
    "        new_diff = np.amax(abs(v_prev - v))\n",
    "        if new_diff > diff: diff = new_diff\n",
    "        \n",
    "        t+=1\n",
    "        if horizon is not None:\n",
    "            if t==horizon: print('break'); break\n",
    "    \n",
    "    return v\n",
    "\n",
    "\n",
    "def compute_policy(mdp, gamma, reward=None, V=None, horizon=None, threshold=1e-4):\n",
    "    \n",
    "    if reward is None and V is None: raise Exception('Cannot compute V: no reward provided')\n",
    "    if V is None: V = compute_value_boltzmann(mdp, gamma, reward, horizon=horizon, threshold=threshold)\n",
    "    print(V)\n",
    "    \n",
    "    policy = np.zeros((mdp.nS, mdp.nA))\n",
    "    for s in range(mdp.nS):\n",
    "        for a in range(mdp.nA):\n",
    "            policy[s,a] = np.exp(r1[s] - V[s] + np.dot(mdp.T[s, a,:], gamma * V))\n",
    "    \n",
    "    if np.sum(np.isnan(policy)) > 0: raise Exception('NaN encountered in policy')\n",
    "    \n",
    "    return policy\n",
    "\n",
    "\n",
    "def generate_trajectories(mdp, policy, T=20, num_traj=50):\n",
    "    s = mdp.env.reset()\n",
    "    \n",
    "    trajectories = np.zeros([num_traj, T, 2]).astype(int)\n",
    "    \n",
    "    for d in range(num_traj):\n",
    "        for t in range(T):\n",
    "            action = np.random.choice(range(mdp.nA), p=policy[s, :])\n",
    "            trajectories[d, t, :] = [s, action]\n",
    "            s, _, _, _ = mdp.env.step(action)\n",
    "        s = mdp.env.reset()\n",
    "    \n",
    "    return trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_irl_log_likelihood(mdp, gamma, trajectories, V, r):\n",
    "\n",
    "    L_D = 0\n",
    "\n",
    "    for traj in trajectories:\n",
    "        for (s, a) in traj:\n",
    "            # This is Q[s,a] - V[s]\n",
    "            L_D += r[s] + np.dot(mdp.T[s,a,:], gamma * V) - V[s]\n",
    "    \n",
    "    return L_D\n",
    "\n",
    "\n",
    "\n",
    "def compute_s_a_visitations(mdp, gamma, trajectories):\n",
    "    \"\"\"\n",
    "    Computes the empirical state and state-action visitation frequencies from \n",
    "    the expert trajectories\n",
    "    \"\"\"\n",
    "    \n",
    "    mu_hat_sa = np.zeros((mdp1.nS, mdp1.nA))\n",
    "    v_hat_s = np.zeros((mdp1.nS))\n",
    "    for traj in trajectories:\n",
    "        for (s, a) in traj:\n",
    "            mu_hat_sa[s, a] += 1\n",
    "            v_hat_s[s] += 1\n",
    "\n",
    "            v_hat_s -= gamma * mdp1.T[s,a,:]\n",
    "            # Same as the line above but slower:\n",
    "            #for (s_prime, p_transition) in enumerate(t1[s,a,:]):\n",
    "            #    v_hat_s[s_prime] -= gamma * p_transition\n",
    "    \n",
    "    v_hat_s = v_hat_s / (trajectories.shape[0] * trajectories.shape[1])\n",
    "    mu_hat_sa = mu_hat_sa / (trajectories.shape[0] * trajectories.shape[1])\n",
    "    if np.sum(np.isnan(mu_hat_sa)) > 0: raise Exception('NaN encountered')\n",
    "    \n",
    "    return(mu_hat_sa, v_hat_s)\n",
    "        \n",
    "        \n",
    "def compute_D_old(mdp, gamma, V, policy, v_hat_s, D = None, threshold = 1e-3):\n",
    "    \"\"\"\n",
    "    Computes occupancy measure of a MDP under a given policy -- \n",
    "    the expected discounted number of times that policy π visits state s.\n",
    "    \"\"\"\n",
    "    assert V.shape[0] == mdp.nS\n",
    "    assert policy.shape == (mdp.nS, mdp.nA)   \n",
    "    \n",
    "    assert np.sum(np.isnan(V)) == 0\n",
    "    assert np.sum(np.isnan(policy)) == 0\n",
    "    assert np.sum(np.isnan(v_hat_s)) == 0\n",
    "        \n",
    "    \n",
    "    if D is None: D = np.tile(v_hat_s, (mdp.nA,1)).T / mdp.nA\n",
    "    \n",
    "    else: D = np.tile(D, (mdp.nA, 1))\n",
    "    \n",
    "    diff = float(\"inf\")\n",
    "    \n",
    "    for t in range(1, 15):\n",
    "    #while diff > threshold:\n",
    "        D_new = np.copy(D)\n",
    "        \n",
    "        for s_prime in range(mdp.nS):\n",
    "            for a_prime in range(mdp.nA):\n",
    "                D_new[s_prime, a_prime] = (policy[s_prime, a_prime] * \n",
    "                                                 (np.sum(gamma * mdp.T[:,:,s_prime] * D)))\n",
    "        \n",
    "        if np.sum(D_new>1e4) > 0: print(t, D_new)\n",
    "        \n",
    "        diff = np.amax(abs(D - D_new))    \n",
    "        D = np.copy(D_new)\n",
    "        \n",
    "    \n",
    "    if np.sum(np.isnan(D)) > 0: raise Exception('NaN encountered in occupancy measure')\n",
    "    return np.sum(D, 1)\n",
    "\n",
    "\n",
    "def compute_D(mdp, gamma, V, policy, v_hat_s, horizon=None, D = None, threshold = 1e-6):\n",
    "    \"\"\"\n",
    "    Computes occupancy measure of a MDP under a given policy -- \n",
    "    the expected discounted number of times that policy π visits state s.\n",
    "    \"\"\"\n",
    "    assert V.shape[0] == mdp.nS\n",
    "    assert policy.shape == (mdp.nS, mdp.nA)   \n",
    "    \n",
    "    assert np.sum(np.isnan(V)) == 0\n",
    "    assert np.sum(np.isnan(policy)) == 0\n",
    "    assert np.sum(np.isnan(v_hat_s)) == 0\n",
    "        \n",
    "    \n",
    "    #if D is None: D =  np.zeros((mdp.nS, mdp.nA), dtype='float64')\n",
    "    #if D is None: D = np.tile(v_hat_s, (mdp.nA,1)).T / mdp.nA\n",
    "    if D is None: D = v_hat_s    \n",
    "    else: D = np.tile(D, (mdp.nA, 1))\n",
    "    \n",
    "    \n",
    "    \n",
    "    if horizon is not None:\n",
    "        gamma = 1\n",
    "        t = 1\n",
    "    \n",
    "    diff = float(\"inf\")\n",
    "    while diff > threshold:\n",
    "        D_new = np.zeros_like(D)\n",
    "        \n",
    "        for s in range(mdp.nS):\n",
    "            for s_prime in range(mdp.nS):\n",
    "                for a_prime in range(mdp.nA):\n",
    "                    D_new[s_prime] += (policy[s, a_prime] \n",
    "                                              * (gamma * mdp.T[s,a_prime,s_prime] * D[s]))\n",
    "\n",
    "            if np.sum(D_new>1e4) > 0: print(t, D_new)\n",
    "        \n",
    "        diff = np.amax(abs(D - D_new))    \n",
    "        D = np.copy(D_new)\n",
    "        \n",
    "        if horizon is not None:\n",
    "            t+=1\n",
    "            if t==horizon: break\n",
    "    \n",
    "    if np.sum(np.isnan(D)) > 0: raise Exception('NaN encountered in occupancy measure')\n",
    "    return D\n",
    "\n",
    "\n",
    "\n",
    "def irl_log_likelihood_and_grad(r, mdp, gamma, trajectories, horizon=None):\n",
    "    \n",
    "    V = compute_value_boltzmann(mdp1, gamma, r, horizon=horizon)\n",
    "    # IRL log likelihood term\n",
    "    L_D = compute_irl_log_likelihood(mdp, gamma, trajectories, V, r)\n",
    "    \n",
    "    # IRL log likelihood gradient w.r.t reward\n",
    "    policy = compute_policy(mdp1, gamma, V=V) \n",
    "    mu_hat, v_hat_s = compute_s_a_visitations(mdp, gamma, trajectories)\n",
    "    \n",
    "    D = compute_D(mdp, gamma, V, policy, v_hat_s, horizon=horizon)\n",
    "    \n",
    "    dL_D_dr = np.sum(mu_hat,1) - D\n",
    "    \n",
    "    return L_D, -dL_D_dr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mdp1 = MDP(FrozenLakeEnvMultigoal(is_slippery=False, goal=1))\n",
    "\n",
    "r1 = np.zeros(64)\n",
    "r1[63] = 1.0\n",
    "\n",
    "policy1 = compute_policy(mdp1, gamma, r1, threshold=1e-8, horizon=None)\n",
    "\n",
    "np.sum(policy1, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "horizon = 15\n",
    "trajectories1 = generate_trajectories(mdp1, policy1, T=horizon, num_traj=200)\n",
    "\n",
    "trajectories1.shape\n",
    "\n",
    "\n",
    "#print(trajectories1[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irl_log_likelihood_and_grad(r1, mdp1, gamma,  trajectories1, horizon=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "r = np.random.rand(64)\n",
    "r[63] = 1\n",
    "print(r)\n",
    "\n",
    "for i in range(epochs):\n",
    "    cost, grad = irl_log_likelihood_and_grad(r, mdp1, gamma,  trajectories1)\n",
    "    r = r - 0.1*grad\n",
    "    print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.arange(64).reshape(64,)\n",
    "\n",
    "np.tile(a, (4,1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "r = np.random.rand(64)\n",
    "minimize(irl_log_likelihood_and_grad, x0=r, args = (mdp1, gamma, trajectories1), jac = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
