{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "intro_1",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# FrozenLake Mixing Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "intro_2",
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "This notebook and the accompanying environment code is adapted from the homework assignment written by John Schulman [here](https://github.com/berkeleydeeprlcourse/homework/tree/master/hw2).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
      "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
      "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
      "    If you step into one of those holes, you'll fall into the freezing water.\n",
      "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
      "    you navigate across the lake and retrieve the disc.\n",
      "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
      "    The surface is described using a grid like the following\n",
      "\n",
      "        SFFF\n",
      "        FHFH\n",
      "        FFFH\n",
      "        HFFG\n",
      "\n",
      "    S : starting point, safe\n",
      "    F : frozen surface, safe\n",
      "    H : hole, fall to your doom\n",
      "    G : goal, where the frisbee is located\n",
      "\n",
      "    The episode ends when you reach the goal or fall in a hole.\n",
      "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from frozen_lake import *\n",
    "env = FrozenLakeEnvMultigoal(goal=2)\n",
    "print(env.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's look at what a random episode looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Some basic imports and setup\n",
    "import numpy as np, numpy.random as nr, gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Seed RNGs so you get the same printouts as me\n",
    "env.seed(0); from gym.spaces import prng; prng.seed(10)\n",
    "\n",
    "NUM_ITER = 75\n",
    "GAMMA = 0.95\n",
    "\n",
    "# Generate the episode\n",
    "# env.reset()\n",
    "# for t in range(100):\n",
    "#     env.render()\n",
    "#     a = env.action_space.sample()\n",
    "#     ob, rew, done, _ = env.step(a)\n",
    "#     if done:\n",
    "#         break\n",
    "# assert done\n",
    "# env.render();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the episodeÂ above, the agent falls into a hole. Also note the stochasticity--on the first step, the DOWN action is selected, but the agent moves to the right.\n",
    "\n",
    "We extract the relevant information from the gym Env into the MDP class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mdp.P is a two-level dict where the first key is the state and the second key is the action.\n",
      "The 2D grid cells are associated with indices [0, 1, 2, ..., 15] from left to right and top to down, as in\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]]\n",
      "mdp.P[state][action] is a list of tuples (probability, nextstate, reward).\n",
      "\n",
      "For example, state 0 is the initial state, and the transition information for s=0, a=0 is \n",
      "P[0][0] = [(0.1, 0, 0.0), (0.8, 0, 0.0), (0.1, 8, 0.0)] \n",
      "\n",
      "As another example, state 5 corresponds to a hole in the ice, which transitions to itself with probability 1 and reward 0.\n",
      "P[5][0] = [(0.1, 5, 0.0), (0.8, 4, 0.0), (0.1, 13, 0.0)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MDP(object):\n",
    "    def __init__(self, env):\n",
    "        P, nS, nA, desc = MDP.env2mdp(env)\n",
    "        self.P = P # state transition and reward probabilities, explained below\n",
    "        self.nS = nS # number of states\n",
    "        self.nA = nA # number of actions\n",
    "        self.desc = desc # 2D array specifying what each grid cell means (used for plotting)\n",
    "        self.env = env\n",
    "\n",
    "    def env2mdp(env):\n",
    "        return {s : {a : [tup[:3] for tup in tups] for (a, tups) in a2d.items()} for (s, a2d) in env.P.items()}, env.nS, env.nA, env.desc\n",
    "    \n",
    "    def get_transition_matrix(self):\n",
    "        \"\"\"Return a matrix with index S,A,S' -> P(S'|S,A)\"\"\"\n",
    "        T = np.zeros([self.nS, self.nA, self.nS])\n",
    "        for s in range(self.nS):\n",
    "            for a in range(self.nA):\n",
    "                transitions = self.P[s][a]\n",
    "                s_a_s = {t[1]:t[0] for t in transitions}\n",
    "                for s_prime in range(self.nS):\n",
    "                    if s_prime in s_a_s:\n",
    "                        T[s, a, s_prime] = s_a_s[s_prime]\n",
    "        return T\n",
    "    \n",
    "mdp = MDP(env)\n",
    "\n",
    "\n",
    "print(\"mdp.P is a two-level dict where the first key is the state and the second key is the action.\")\n",
    "print(\"The 2D grid cells are associated with indices [0, 1, 2, ..., 15] from left to right and top to down, as in\")\n",
    "print(np.arange(16).reshape(4,4))\n",
    "print(\"mdp.P[state][action] is a list of tuples (probability, nextstate, reward).\\n\")\n",
    "print(\"For example, state 0 is the initial state, and the transition information for s=0, a=0 is \\nP[0][0] =\", mdp.P[0][0], \"\\n\")\n",
    "print(\"As another example, state 5 corresponds to a hole in the ice, which transitions to itself with probability 1 and reward 0.\")\n",
    "print(\"P[5][0] =\", mdp.P[5][0], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Code for Mixing Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_traj_likelihood(mdp, traj, actions):\n",
    "    \"\"\"\n",
    "    Naive likelihood calculation. Should work ok for simple gridworld with short trajectories\n",
    "    \"\"\"\n",
    "    transitions = [(traj[i], actions[i], traj[i+1]) for i in range(len(traj)-1)]\n",
    "    likelihood = 1.0\n",
    "    for s,a,s_prime in transitions:\n",
    "        t = mdp.P[s][a]\n",
    "        transition_prob = 0.0\n",
    "        for p, nextstate, _ in t:\n",
    "            if nextstate == s_prime:\n",
    "                transition_prob = p\n",
    "        likelihood *= transition_prob\n",
    "    return likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_Q(T_s, V, gamma):\n",
    "            Q = {}\n",
    "            for a in T_s:\n",
    "                Q_sa = sum(\n",
    "                    [sas[0]*(sas[2] + gamma*V[sas[1]]) for sas in T_s[a]])\n",
    "                Q[a] = Q_sa\n",
    "            return Q\n",
    "\n",
    "def compute_values(mdp1, mdp2, gamma, nIt):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        mdp: MDP\n",
    "        gamma: discount factor\n",
    "        nIt: number of iterations, corresponding to n above\n",
    "    Outputs:\n",
    "        (value_functions, policies)\n",
    "        \n",
    "    len(value_functions) == nIt+1 and len(policies) == n\n",
    "    \"\"\"\n",
    "    V1s = [np.zeros(mdp1.nS)] # list of value functions contains the initial value function, which is zero\n",
    "    V2s = [np.zeros(mdp2.nS)] # list of value functions contains the initial value function, which is zero\n",
    "    pis = []\n",
    "    for it in range(nIt):\n",
    "        Vprev1 = V1s[-1]\n",
    "        Vprev2 = V2s[-1]\n",
    "        \n",
    "        # Your code should define variables V: the bellman backup applied to Vprev\n",
    "        # and pi: the greedy policy applied to Vprev\n",
    "        \n",
    "        \n",
    "        V1 = np.zeros(mdp1.nS)\n",
    "        V2 = np.zeros(mdp2.nS)\n",
    "\n",
    "        for s in range(mdp1.nS):\n",
    "            Q1 = compute_Q(mdp1.P[s], Vprev1, gamma)\n",
    "            Q2 = compute_Q(mdp2.P[s], Vprev2, gamma)\n",
    "            V1[s] = max(Q1.values())\n",
    "            V2[s] = max(Q2.values())\n",
    "        \n",
    "        V1s.append(V1)\n",
    "        V2s.append(V2)\n",
    "    return V1s[-1], V2s[-1]\n",
    "    \n",
    "    \n",
    "def mix_values(V1, V2, w1=1.0, w2=1.0, gamma=GAMMA):\n",
    "    V = np.zeros(mdp1.nS)\n",
    "    pi = np.zeros(mdp.nS)\n",
    "    for s in range(mdp1.nS):\n",
    "        Q1_s = compute_Q(mdp1.P[s], V1, gamma)\n",
    "        Q2_s = compute_Q(mdp2.P[s], V2, gamma)\n",
    "        mixed_q_s = {a: w1*Q1_s[a] + w2*Q2_s[a] for a in Q1_s}\n",
    "        V[s] = max(mixed_q_s.values())\n",
    "        pi[s] = max(mixed_q_s, key=lambda x:mixed_q_s[x])\n",
    "    return V, pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def normalize(*args):\n",
    "    normalized = []\n",
    "    total = sum(args)\n",
    "    for arg in args:\n",
    "        normalized.append(arg / total)\n",
    "    return tuple(normalized)\n",
    "\n",
    "def plot_values(V, pi, s=None):\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(V.reshape(8,8), cmap='gray', interpolation='none')#, clim=(0,1))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(8)-.5)\n",
    "    ax.set_yticks(np.arange(8)-.5)\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    Y, X = np.mgrid[0:8, 0:8]\n",
    "    a2uv = {0: (-1, 0), 1:(0, -1), 2:(1,0), 3:(0, 1)}\n",
    "    Pi = pi.reshape(8,8)\n",
    "    for y in range(8):\n",
    "        for x in range(8):\n",
    "            a = Pi[y, x]\n",
    "            u, v = a2uv[a]\n",
    "            plt.arrow(x, y,u*.3, -v*.3, color='m', head_width=0.1, head_length=0.1) \n",
    "            plt.text(x, y, str(env.desc[y,x].item().decode()),\n",
    "                     color='g', size=12,  verticalalignment='center',\n",
    "                     horizontalalignment='center', fontweight='bold')\n",
    "    if s != None:\n",
    "        plt.plot(s%8, s//8, 'ro')\n",
    "    plt.grid(color='b', lw=2, ls='-')\n",
    "    \n",
    "def run_mixing_agent(true_mdp, mdp1, mdp2, p1=1.0, p2=1.0, vis=plot_values):\n",
    "    s1 = mdp1.env.reset()\n",
    "    s2 = mdp2.env.reset()\n",
    "    assert s1 == s2, \"mdp state spaces must be the same\"\n",
    "    s, done = s1, False\n",
    "    \n",
    "    Vs = []\n",
    "    pi = np.zeros(mdp1.nS)\n",
    "    \n",
    "    w1, w2 = normalize(p1, p2)\n",
    "    traj, actions = [s,], []\n",
    "    V1, V2 = compute_values(mdp1, mdp2, GAMMA, NUM_ITER)\n",
    "\n",
    "    while not done:\n",
    "        V, pi = mix_values(V1, V2, w1, w2, GAMMA)\n",
    "        action = pi[s]\n",
    "        s, _, done, _ = true_mdp.env.step(action)\n",
    "        traj.append(s)\n",
    "        actions.append(action)\n",
    "        l1, l2 = compute_traj_likelihood(mdp1, traj, actions), compute_traj_likelihood(mdp2, traj, actions)\n",
    "        w1, w2 = normalize(p1*l1, p2*l2)\n",
    "        if vis:\n",
    "            vis(V, pi, s)\n",
    "    return traj\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Experiments\n",
    "\n",
    "These experiments test a set of simplistic cases for the mixing agent. Player 1 wants to go to goal 1, player 2 wants to go to goal 2. In each of these experiments, one of the players has the correct belief about the environment, and we should see the mixing agent learn to go to the goal corresponding to the correct agent.\n",
    "\n",
    "The visualizations show the mixing agent's estimated value of the states after each action.\n",
    "\n",
    "**Experiment 1:** Player 1 believs the environment is determinsitic. Player 2 believes it isn't. The true mdp is deterministic, so the mixing agent should eventually learn to go to goal 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mdp1 = MDP(FrozenLakeEnvMultigoal(is_slippery=False, goal=1))\n",
    "mdp2 = MDP(FrozenLakeEnvMultigoal(is_slippery=True, goal=2))\n",
    "true_mdp = MDP(FrozenLakeEnvMultigoal(is_slippery=False, goal=1))\n",
    "\n",
    "run_mixing_agent(true_mdp, mdp1, mdp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Experiment 2:** Player 1 believs the environment is determinsitic. Player 2 believes it isn't. The true mdp is *not* deterministic, so the mixing agent should quickly learn to go to goal 2.\n",
    "\n",
    "- **Note:** This is a pathological case where one player assigns probability 0 to a realized trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mdp1 = MDP(FrozenLakeEnvMultigoal(is_slippery=False, goal=2))\n",
    "mdp2 = MDP(FrozenLakeEnvMultigoal(is_slippery=True, goal=1))\n",
    "true_mdp = MDP(FrozenLakeEnvMultigoal(is_slippery=True))\n",
    "\n",
    "run_mixing_agent(true_mdp, mdp1, mdp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Experiment 3:** Both players have the same (true) belief, but player 2 has a higher prior value. The mixing agent should go to goal 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mdp1 = MDP(FrozenLakeEnvMultigoal(is_slippery=True, goal=1))\n",
    "mdp2 = MDP(FrozenLakeEnvMultigoal(is_slippery=True, goal=2))\n",
    "true_mdp = MDP(FrozenLakeEnvMultigoal(is_slippery=True))\n",
    "\n",
    "run_mixing_agent(true_mdp, mdp1, mdp2, 2.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from irl import maxent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mdp1 = MDP(FrozenLakeEnvMultigoal(is_slippery=False, goal=1))\n",
    "mdp2 = MDP(FrozenLakeEnvMultigoal(is_slippery=True, goal=2))\n",
    "true_mdp = MDP(FrozenLakeEnvMultigoal(is_slippery=False, goal=1))\n",
    "\n",
    "feature_matrix = np.eye(mdp1.nS)\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "r1 = np.zeros(64)\n",
    "r2 = np.zeros(64)\n",
    "r1[63] = 1.0\n",
    "r2[7] = 1.0\n",
    "\n",
    "[r1, r2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "t1 = mdp1.get_transition_matrix()\n",
    "t2 = mdp2.get_transition_matrix()\n",
    "t1, t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "policy1 = maxent.find_policy(mdp1.nS, r1, mdp1.nA, GAMMA, t1)\n",
    "policy2 = maxent.find_policy(mdp2.nS, r2, mdp2.nA, GAMMA, t2)\n",
    "policy1, policy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def generate_trajectories(mdp, policy, T=20, D=50):\n",
    "    s = mdp.env.reset()\n",
    "    \n",
    "    trajectories = np.zeros([D, T, 2]).astype(int)\n",
    "    \n",
    "    for d in range(D):\n",
    "        for t in range(T):\n",
    "            action = np.random.choice(range(mdp.nA), p=policy[s, :])\n",
    "            trajectories[d, t, :] = [s, action]\n",
    "            s, _, _, _ = mdp.env.step(action)\n",
    "        s = mdp.env.reset()\n",
    "    \n",
    "    return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "trajectories1 = generate_trajectories(mdp1, policy1, T=50, D=100)\n",
    "trajectories2 = generate_trajectories(mdp2, policy2, T=50, D=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "maxent_r1 = maxent.irl(feature_matrix=feature_matrix, n_actions=mdp1.nA, discount=GAMMA, transition_probability=t1, trajectories=trajectories1, epochs=100, learning_rate=0.01)\n",
    "maxent_r2 = maxent.irl(feature_matrix=feature_matrix, n_actions=mdp2.nA, discount=GAMMA, transition_probability=t2, trajectories=trajectories2, epochs=100, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "maxent_r1 -= np.min(maxent_r1)\n",
    "maxent_r1 /= np.max(maxent_r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "maxent_r2 -= np.min(maxent_r2)\n",
    "maxent_r2 /= np.max(maxent_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def replace_p(mdp, new_r):\n",
    "    new_p = copy.deepcopy(mdp.P)\n",
    "    for s in range(mdp.nS):\n",
    "        for a in range(mdp.nA):\n",
    "            transitions = mdp.P[s][a]\n",
    "            new_transitions = [(t[0] ,t[1], new_r[int(t[1])]) for t in transitions]\n",
    "            new_p[s][a] = new_transitions\n",
    "    mdp.P = new_p\n",
    "\n",
    "replace_p(mdp1, maxent_r1)\n",
    "replace_p(mdp2, maxent_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "maxent_r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "maxent_r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "run_mixing_agent(true_mdp, mdp1, mdp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multigoal Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'in <string>' requires string as left operand, not numpy.bytes_",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8e0fff098d64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmdp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMDP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrozenLakeEnvSequentialMultigoal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_slippery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmdp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMDP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrozenLakeEnvSequentialMultigoal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_slippery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrue_mdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMDP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrozenLakeEnvSequentialMultigoal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_slippery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtraj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_mixing_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_mdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/nishant/Projects/irl/frozenlake/frozen_lake.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, desc, map_name, is_slippery, goal)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mncol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'12'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m                     \u001b[0mgoal_inds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgoal_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                     \u001b[0mgoal_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not numpy.bytes_"
     ]
    }
   ],
   "source": [
    "mdp1 = MDP(FrozenLakeEnvSequentialMultigoal(is_slippery=False, goal=1))\n",
    "mdp2 = MDP(FrozenLakeEnvSequentialMultigoal(is_slippery=False, goal=2))\n",
    "true_mdp = MDP(FrozenLakeEnvSequentialMultigoal(is_slippery=False, goal=1))\n",
    "\n",
    "traj = run_mixing_agent(true_mdp, mdp1, mdp2, w1=1.0, w2=0.0, GAMMA=1.0, vis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
